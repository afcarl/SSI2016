{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Day 4\n",
    "* Preliminaries\n",
    "* The philosophy of machine learning\n",
    "* Machine learning in Python: why scikit-learn?\n",
    "* Feature extraction\n",
    "* Feature selection\n",
    "* Estimation\n",
    "* Evaluation\n",
    "* Automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tal/anaconda/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "# We'll load scikit-learn modules as we go,\n",
    "# so we can see what we're using.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in our preprocessed data set from Day 3.\n",
    "# You may need to modify the path to the file\n",
    "# depending on where you put it on your computer.\n",
    "data = pd.read_csv('../data/preprocessed_data.csv')\n",
    "\n",
    "# Since we'll be predicting outcomes, let's restrict\n",
    "# to only common ones. It's hard to predict something\n",
    "# we don't have very many training examples of.\n",
    "data = data.groupby('outcome').filter(lambda x: len(x) >= 500)\n",
    "\n",
    "# Let's also do some recoding to make life easier\n",
    "data = data.dropna(subset=['age'])\n",
    "categoricals = ['sex', 'sterilized']\n",
    "data[categoricals] = data[categoricals].fillna('Unknown')\n",
    "\n",
    "# Important, otherwise we have problems later\n",
    "# when we try to concatenate based on index\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The philosophical foundations of machine learning\n",
    "* There aren't really (m)any\n",
    "* Machine learning is an almost ruthlessly pragmatic field\n",
    "* Good prediction is (usually) the main focus of analysis\n",
    "    * Understanding only matters to the extent it helps prediction\n",
    "* This makes iterative model-fitting much more rapid\n",
    "    * Don't need to understand _why_ something works, just need to feel confident that it _does_ work\n",
    "* Places a premium on objective tests of predictive accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning in Python: why scikit-learn?\n",
    "* There are hundreds of ML packages in Python\n",
    "    * Theano, Tensorflow, orange, Pattern, PyMVPA, etc...\n",
    "* But scikit-learn is dominant\n",
    "    * Elegant, powerful interface\n",
    "    * World-class documentation\n",
    "    * Excellent performance\n",
    "* The exception is deep learning--not supported in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The typical predictive modeling pipeline\n",
    "* Feature extraction/engineering\n",
    "* Feature selection\n",
    "* Model/parameter selection\n",
    "* Estimation\n",
    "* Evaluation\n",
    "* Rinse and repeat ad nauseam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature extraction\n",
    "* The process of transforming data into more data\n",
    "* We already did lot of this on Day 2\n",
    "* But let's revisit the issue in this new predictive light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting _all_ the features\n",
    "* Well, probably not all... but a _lot_\n",
    "* How much information can we get out of the original dataset?\n",
    "* From an interpretation-oriented standpoint, maybe not much more\n",
    "* From a machine learning standpoint, we've just scratched the surface\n",
    "* Some things we could add: names, colors, any number of interactions...\n",
    "    * The cost of trying out silly things is much lower\n",
    "    * Multicollinearity is not (much of) a concern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The bag-of-words model\n",
    "* If you have a lot of data, it's not always worth thinking deeply about your features\n",
    "* E.g., how should we model fur color?\n",
    "    * \"Black/Tricolor\", \"Calico Point\", \"Brown Brindle/Blue Cream\"\n",
    "* Simple approach: treat color descriptions like a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "* Extract all word tokens (possibly even N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Yellow/White', 'Tricolor', 'Brown/White', 'Black/White',\n",
       "       'Black/Gray', 'White/Black', 'Brown Tabby/White', 'Black/Tan',\n",
       "       'Tan/Black', 'Black', 'Gold/Gold', 'Sable/White', 'Blue Merle/Tan',\n",
       "       'Blue Merle', 'Brown/Black', 'Black/Tricolor', 'Black/Black', 'Tan',\n",
       "       'Cream', 'Red'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 20 unique colors in the dataset\n",
    "data['color'].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applying the bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fur_features is an object of type: <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# The CountVectorizer is an estimator that takes a series\n",
    "# of documents (or strings) as input, and returns a count\n",
    "# of every word token found in every document. There's also\n",
    "# a TfidfVectorizer in cases where we want normalized frequency.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with default parameters.\n",
    "# Some common arguments we might want to experiment with\n",
    "# include min_df and max_df (which exclude words that are\n",
    "# too frequent or infrequent), stop_words (which allows\n",
    "# us to pass in a list of words to ignore), and ngram_range,\n",
    "# which enables us to extract multi-word features.\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# Extract all possible word features from the color list.\n",
    "# Note that this returns a sparse matrix rather than a\n",
    "# numpy array or a pandas DataFrame. A sparse matrix is\n",
    "# a way of representing potentially very large 2-d arrays\n",
    "# very efficiently, because we don't need to allocate\n",
    "# memory for every cell in the array, only those that\n",
    "# have a non-zero value.\n",
    "fur_features = vec.fit_transform(data['color'])\n",
    "print(\"fur_features is an object of type:\", type(fur_features))\n",
    "\n",
    "# After fitting, the names of the features (i.e., the\n",
    "# columns of the sparse matrix returned by fit_transform())\n",
    "# are stored in the estimator itself.\n",
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "# Store in a pandas DF for easier manipulation later.\n",
    "# Note that we convert the sparse array back to a dense\n",
    "# one before loading into pandas. If our dataset were\n",
    "# much bigger, we'd probably want to avoid this step\n",
    "# and just keep working with the sparse matrix.\n",
    "fur_features = pd.DataFrame(fur_features.todense(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white        17625\n",
       "black        12305\n",
       "brown        10606\n",
       "tabby         7294\n",
       "tan           4393\n",
       "blue          3378\n",
       "orange        2018\n",
       "brindle       1484\n",
       "red           1475\n",
       "tricolor      1321\n",
       "gray           996\n",
       "cream          911\n",
       "tortie         873\n",
       "point          848\n",
       "calico         798\n",
       "chocolate      746\n",
       "torbie         562\n",
       "sable          485\n",
       "merle          458\n",
       "yellow         443\n",
       "buff           439\n",
       "fawn           299\n",
       "lynx           281\n",
       "seal           235\n",
       "silver         180\n",
       "tick           156\n",
       "smoke          120\n",
       "flame          119\n",
       "gold           114\n",
       "lilac           58\n",
       "apricot         47\n",
       "liver           45\n",
       "green           33\n",
       "pink            27\n",
       "tiger           23\n",
       "agouti           6\n",
       "ruddy            1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look...\n",
    "# fur_features.head(10)\n",
    "fur_features.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding interaction terms\n",
    "* We could dummy-code all our categorical variables and then take pairwise products\n",
    "* But if we don't need interpretabilty, there's a simpler hack\n",
    "    * Concatenate all variables for which we want interactions\n",
    "    * Dummy-code the result\n",
    "* Let's cross sex, sterilization, and breed (that's a lot of features!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique values: 3150\n"
     ]
    }
   ],
   "source": [
    "# Simply concatenate the columns we want--making sure to convert any numeric\n",
    "# columns to string, otherwise the concatenation will fail.\n",
    "data['ssb'] = data['sex'].astype(str) + '_' + data['sterilized'].astype(str) + '_' + data['breed']\n",
    "\n",
    "# How many unique levels?\n",
    "num_levels = data['ssb'].nunique()\n",
    "print(\"Total number of unique values: {}\".format(num_levels))\n",
    "\n",
    "# Now we can dummy-code the result\n",
    "# pd.get_dummies(data['ssb']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What else?\n",
    "* Very easy to quickly build up thousands of derivative features in this way\n",
    "* Doesn't mean we shouldn't think deeply about good features\n",
    "    * Often, biggest jumps in performance are achieved by adding entirely new features\n",
    "* Point is try to eke out every bit of signal from what we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection/reduction\n",
    "* Not all features are created equal\n",
    "* Just because we created 10,000 features doesn't mean we need to include them all\n",
    "* Two general approaches:\n",
    "    * Dimensionality reduction (extract latent signal from observed features)\n",
    "    * Feature selection (filter out features based on some criterion)\n",
    "* Supported by the feature_selection, clustering, and decomposition modules in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decomposition\n",
    "* Techniques for decomposing/factoring matrices\n",
    "    * [sklearn.decomposition](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)\n",
    "    * PCA, ICA, dictionary learning, many others\n",
    "* Reduce dimensionality by extracting signal common to multiple features\n",
    "* Can we reduce our 3,000+ strong set of features to a smaller number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to recode our string column as a set of dummies\n",
    "interaction_dummies = pd.get_dummies(data['ssb'])\n",
    "\n",
    "# Let's concatenate this with the color features\n",
    "# axis=1 indicates that we want to concatenate along\n",
    "# the column axis (axis=0 would append each dataframe\n",
    "# below the last.\n",
    "lotsa_features = pd.concat([fur_features, interaction_dummies], axis=1)\n",
    "\n",
    "# Like most other things in sklearn, decomposition classes\n",
    "# implement the estimator interface. So they have fit() and\n",
    "# predict() methods. Transformers also have a transform()\n",
    "# method. First, we initialize the RandomizedPCA transformer.\n",
    "# RandomizedPCA is a speedier approximation of the standard\n",
    "# principal component analysis (PCA) factorization. We need to\n",
    "# specify the number of components we want at initialization.\n",
    "# We'll take the first 100.\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "pca = RandomizedPCA(100)\n",
    "\n",
    "# Now we can fit and transform in one step\n",
    "pca_features = pca.fit_transform(lotsa_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All components collectively explain 87% of the variance.\n"
     ]
    }
   ],
   "source": [
    "# How much of the variance do these components explain?\n",
    "exp_variance = pca.explained_variance_ratio_.sum()\n",
    "msg = \"All components collectively explain {:.0%} of the variance.\"\n",
    "print(msg.format(exp_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11cc8f898>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAECCAYAAAD3vwBsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aa59e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's take a look at the scree plot\n",
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brown                                     0.530145\n",
       "tabby                                     0.494696\n",
       "orange                                    0.096667\n",
       "0.0_1.0_Domestic Shorthair Mix            0.094312\n",
       "1.0_1.0_Domestic Shorthair Mix            0.065426\n",
       "0.0_0.0_Domestic Shorthair Mix            0.051435\n",
       "blue                                      0.040949\n",
       "1.0_0.0_Domestic Shorthair Mix            0.040882\n",
       "brindle                                   0.036645\n",
       "Unknown_Unknown_Bat Mix                   0.021899\n",
       "Unknown_Unknown_Domestic Shorthair Mix    0.020532\n",
       "cream                                     0.018821\n",
       "0.0_1.0_Domestic Medium Hair Mix          0.007119\n",
       "Unknown_Unknown_Bat                       0.006663\n",
       "tortie                                    0.005310\n",
       "calico                                    0.004794\n",
       "1.0_0.0_Domestic Medium Hair Mix          0.003891\n",
       "0.0_0.0_Domestic Medium Hair Mix          0.003846\n",
       "merle                                     0.003222\n",
       "torbie                                    0.002352\n",
       "0.0_1.0_Domestic Longhair Mix             0.002194\n",
       "0.0_1.0_Maine Coon Mix                    0.002119\n",
       "point                                     0.002065\n",
       "1.0_1.0_Domestic Medium Hair Mix          0.001600\n",
       "silver                                    0.001494\n",
       "1.0_1.0_Plott Hound Mix                   0.001480\n",
       "1.0_1.0_Maine Coon Mix                    0.001379\n",
       "Unknown_Unknown_Squirrel Mix              0.001014\n",
       "0.0_0.0_Domestic Longhair Mix             0.000960\n",
       "0.0_1.0_Boxer Mix                         0.000900\n",
       "                                            ...   \n",
       "1.0_1.0_Rat Terrier Mix                  -0.002331\n",
       "0.0_1.0_Rottweiler Mix                   -0.002526\n",
       "0.0_1.0_Miniature Schnauzer Mix          -0.002660\n",
       "1.0_1.0_Australian Cattle Dog Mix        -0.002721\n",
       "0.0_0.0_Pit Bull Mix                     -0.002814\n",
       "1.0_0.0_Chihuahua Shorthair Mix          -0.002821\n",
       "0.0_1.0_Miniature Poodle Mix             -0.002900\n",
       "Unknown_Unknown_Skunk Mix                -0.002916\n",
       "fawn                                     -0.003072\n",
       "Unknown_Unknown_Raccoon                  -0.003518\n",
       "0.0_0.0_Labrador Retriever Mix           -0.003537\n",
       "1.0_0.0_Labrador Retriever Mix           -0.004404\n",
       "0.0_1.0_Border Collie Mix                -0.005302\n",
       "smoke                                    -0.005569\n",
       "chocolate                                -0.005674\n",
       "1.0_1.0_German Shepherd Mix              -0.005799\n",
       "0.0_1.0_German Shepherd Mix              -0.006112\n",
       "1.0_1.0_Border Collie Mix                -0.006571\n",
       "0.0_1.0_Pit Bull Mix                     -0.006901\n",
       "1.0_1.0_Pit Bull Mix                     -0.006986\n",
       "Unknown_Unknown_Raccoon Mix              -0.009486\n",
       "1.0_1.0_Chihuahua Shorthair Mix          -0.009530\n",
       "0.0_1.0_Chihuahua Shorthair Mix          -0.010910\n",
       "red                                      -0.013245\n",
       "gray                                     -0.015488\n",
       "1.0_1.0_Labrador Retriever Mix           -0.021240\n",
       "0.0_1.0_Labrador Retriever Mix           -0.024344\n",
       "tan                                      -0.119819\n",
       "white                                    -0.187904\n",
       "black                                    -0.625226\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at the loadings if we want to try to understand what's going on\n",
    "components = pca.components_\n",
    "loadings = pd.DataFrame(components, columns=lotsa_features.columns)\n",
    "loadings.iloc[0].sort_values(ascending=False)\n",
    "\n",
    "# As an exercise, you can come up with a better way to visualize what's\n",
    "# going on. For example, you could pick out some of the most common\n",
    "# features in the original space (by summing each of the columns over\n",
    "# the rows), and then plot a heatmap of loadings of the first 10\n",
    "# PCA components against the 10 most commonly occurring features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Clustering\n",
    "* The goal of clustering is to identify homogeneous clusters of cases in the data\n",
    "* Ideally, these clusters align well with clear natural boundaries\n",
    "    * E.g., clustering the animal data produces a cat cluster, dog cluster, etc.\n",
    "* With real-world data, they rarely do\n",
    "* Clustering is a convenient way to describe data, **not** a window onto underlying reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Illustration of clustering methods (from the [scikit-learn docs](http://scikit-learn.org/stable/modules/clustering.html))\n",
    "<img src=\"http://scikit-learn.org/stable/_images/plot_cluster_comparison_0011.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What features should we cluster on?\n",
    "* There is no \"true\" clustering; everything depends on the features we pick\n",
    "* Clustering can get very slow for large n_samples and/or n_features\n",
    "* Often (as in this case) doesn't make sense in presence of mostly categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting features\n",
    "* The preceding approaches all involve selecting or creating new features\n",
    "* But we could also just keep some and throw out others\n",
    "* Many things we can select for:\n",
    "    - Keep high-variance features\n",
    "    - Keep best-scoring features (i.e., strongest correlation with outcome)\n",
    "    - Fit a preliminary estimator like lasso that includes feature selection\n",
    "    - etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use \"KBest\" feature selection, which simply retains the\n",
    "# top K features, as ranked by their correlation with the\n",
    "# outcome of interest. There are plenty of other options\n",
    "# inside the feature_selection module. Note that because\n",
    "# there are multiple classes in our outcome variable, we'll\n",
    "# need to first extract a performance metric we can rank.\n",
    "# For that, we can use the chi2 scoring function, which gives\n",
    "# us the chi2 score for the outcome against each feature.\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Let's pick 100 features, to match the PCA above.\n",
    "# Notice that sklearn accepts a scoring function as the\n",
    "# first argument, so we don't even have to compute the chi2\n",
    "# scores ourselves--we just pass the ranking method directly\n",
    "# to the SelectKBest instance!\n",
    "slctr = SelectKBest(chi2, k=100)\n",
    "\n",
    "# We do, however, need to convert the outcome string to\n",
    "# integer class labels. Fortunately, sklearn has a utility\n",
    "# for that. The LabelEncoder is also an estimator, so\n",
    "# we need to initialize it before we fit_transform.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(data['outcome']) \n",
    "\n",
    "# We'll apply to the same set of color and sex/sterilized/breed features\n",
    "selected_features = slctr.fit_transform(lotsa_features, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see what features were kept..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['blue', 'brindle', 'brown', 'buff', 'calico', 'chocolate', 'gray',\n",
       "       'liver', 'merle', 'orange', 'point', 'red', 'sable', 'smoke', 'tabby',\n",
       "       'tan', 'torbie', 'tortie', 'tricolor', 'white', 'yellow', '0.0_0.0_Bat',\n",
       "       '0.0_0.0_Boxer Mix', '0.0_0.0_Chihuahua Shorthair Mix',\n",
       "       '0.0_0.0_Domestic Longhair Mix', '0.0_0.0_Domestic Medium Hair Mix',\n",
       "       '0.0_0.0_Domestic Shorthair', '0.0_0.0_Domestic Shorthair Mix',\n",
       "       '0.0_0.0_German Shepherd Mix', '0.0_0.0_Labrador Retriever Mix',\n",
       "       '0.0_0.0_Opossum Mix', '0.0_0.0_Pit Bull Mix', '0.0_0.0_Rabbit Sh Mix',\n",
       "       '0.0_0.0_Raccoon', '0.0_0.0_Raccoon Mix', '0.0_0.0_Siamese Mix',\n",
       "       '0.0_1.0_Australian Cattle Dog Mix', '0.0_1.0_Australian Shepherd Mix',\n",
       "       '0.0_1.0_Basset Hound Mix', '0.0_1.0_Boston Terrier Mix',\n",
       "       '0.0_1.0_Chihuahua Shorthair Mix', '0.0_1.0_Domestic Medium Hair Mix',\n",
       "       '0.0_1.0_Domestic Shorthair Mix', '0.0_1.0_German Shepherd Mix',\n",
       "       '0.0_1.0_Golden Retriever Mix', '0.0_1.0_Labrador Retriever',\n",
       "       '0.0_1.0_Labrador Retriever Mix', '0.0_1.0_Miniature Poodle Mix',\n",
       "       '0.0_1.0_Miniature Schnauzer Mix', '0.0_1.0_Pit Bull Mix',\n",
       "       '0.0_1.0_Shih Tzu Mix', '0.0_1.0_Siamese Mix',\n",
       "       '0.0_1.0_Siberian Husky Mix', '1.0_0.0_Chihuahua Shorthair Mix',\n",
       "       '1.0_0.0_Domestic Longhair Mix', '1.0_0.0_Domestic Medium Hair Mix',\n",
       "       '1.0_0.0_Domestic Shorthair Mix', '1.0_0.0_German Shepherd Mix',\n",
       "       '1.0_0.0_Labrador Retriever Mix', '1.0_0.0_Opossum',\n",
       "       '1.0_0.0_Pit Bull Mix', '1.0_0.0_Rabbit Sh Mix', '1.0_0.0_Raccoon Mix',\n",
       "       '1.0_0.0_Siamese Mix', '1.0_0.0_Squirrel Mix',\n",
       "       '1.0_1.0_Australian Cattle Dog Mix', '1.0_1.0_Australian Kelpie Mix',\n",
       "       '1.0_1.0_Australian Shepherd Mix', '1.0_1.0_Border Collie Mix',\n",
       "       '1.0_1.0_Chihuahua Longhair Mix', '1.0_1.0_Chihuahua Shorthair Mix',\n",
       "       '1.0_1.0_Chihuahua Shorthair/Dachshund', '1.0_1.0_Dachshund Mix',\n",
       "       '1.0_1.0_Domestic Longhair Mix', '1.0_1.0_Domestic Medium Hair Mix',\n",
       "       '1.0_1.0_Domestic Shorthair Mix', '1.0_1.0_German Shepherd Mix',\n",
       "       '1.0_1.0_Labrador Retriever Mix', '1.0_1.0_Miniature Poodle Mix',\n",
       "       '1.0_1.0_Pit Bull Mix', '1.0_1.0_Siamese Mix',\n",
       "       '1.0_1.0_Siberian Husky Mix', 'Unknown_Unknown_Bat',\n",
       "       'Unknown_Unknown_Bat Mix', 'Unknown_Unknown_Domestic Longhair Mix',\n",
       "       'Unknown_Unknown_Domestic Medium Hair Mix',\n",
       "       'Unknown_Unknown_Domestic Shorthair Mix', 'Unknown_Unknown_Fox',\n",
       "       'Unknown_Unknown_Fox Mix', 'Unknown_Unknown_Labrador Retriever Mix',\n",
       "       'Unknown_Unknown_Opossum', 'Unknown_Unknown_Opossum Mix',\n",
       "       'Unknown_Unknown_Rabbit Sh Mix', 'Unknown_Unknown_Raccoon',\n",
       "       'Unknown_Unknown_Raccoon Mix', 'Unknown_Unknown_Siamese Mix',\n",
       "       'Unknown_Unknown_Skunk', 'Unknown_Unknown_Skunk Mix',\n",
       "       'Unknown_Unknown_Squirrel', 'Unknown_Unknown_Squirrel Mix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After transformation, the names of the selected features\n",
    "# will be stored as a boolean array in the estimator.\n",
    "# We can index the pandas DataFrame column with this\n",
    "# array to see which features were kept.\n",
    "selected_inds = slctr.get_support()\n",
    "lotsa_features.columns[selected_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimation\n",
    "* With features in hand, we can now fit some models!\n",
    "* scikit-learn has a bewildering array of models\n",
    "* We'll talk about model selection shortly\n",
    "* For now, let's stick with logistic regression\n",
    "    * We'll try to predict animal outcomes again\n",
    "    * A twist: we're now doing multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tal/anaconda/lib/python3.5/site-packages/sklearn/utils/class_weight.py:62: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n",
      "  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='auto', dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our logistic regression estimator\n",
    "est = LogisticRegression(class_weight='auto')\n",
    "\n",
    "# Let's start with a really simple model: predicting\n",
    "# outcome from just the animal's age (plus an intercept,\n",
    "# which is added automatically).\n",
    "X = data[['age']]\n",
    "\n",
    "# We already did this above, but let's do it again,\n",
    "# just so it's clear where y is coming from.\n",
    "y = LabelEncoder().fit_transform(data['outcome'])\n",
    "\n",
    "# Fitting the model is as simple as passing\n",
    "# the X and y arrays to fit().\n",
    "est.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How did we do?\n",
    "* Our model is fitted! Let's see how accurately we classified outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall classification accuracy: 41%\n"
     ]
    }
   ],
   "source": [
    "# Predict y from the trained model. Note that\n",
    "# scikit-learn will not do this for you automatically.\n",
    "# So we call predict(), passing in only the X this\n",
    "# time (since we've already trained on y).\n",
    "y_predicted = est.predict(X)\n",
    "\n",
    "# Now we have a variety of evaluation metrics.\n",
    "# We'll talk about those below. The simplest one is\n",
    "# to just look at the number of cases we classified\n",
    "# correctly.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Most scoring functions in metrics take the true\n",
    "# labels and the predicted labels, in that order.\n",
    "scores = accuracy_score(y, y_predicted)\n",
    "print(\"Overall classification accuracy: {:.0%}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Is this good?\n",
    "* Seems good, no?\n",
    "* There are 4 classes, so chance should be 25%, right?\n",
    "* Or should it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adoption           0.388886\n",
       "Transfer           0.344056\n",
       "Return to Owner    0.169905\n",
       "Euthanasia         0.097153\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the relative class frequencies\n",
    "counts = data['outcome'].value_counts()\n",
    "counts/counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "* Evaluating model performance is rarely straightforward\n",
    "* There are many criteria we might value\n",
    "* Simple answers can be misleading\n",
    "* Let's take a look at _how_ we classified different outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion matrix\n",
    "* How does the classifier go wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAD9CAYAAAAVk8j6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b2493c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y, y_predicted):\n",
    "    ''' take true and predicted scores and plot confusion matrix '''\n",
    "    # Get the confusion matrix\n",
    "    cm = confusion_matrix(y, y_predicted)\n",
    "\n",
    "    # Normalize the confusion matrix by dividing each row by its sum\n",
    "    ncm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Put into DataFrame and get all labels from the encoder\n",
    "    class_labels = encoder.classes_\n",
    "    ncm = pd.DataFrame(ncm, index=class_labels, columns=class_labels)\n",
    "\n",
    "    # Rows are true classes, columns are assigned classes\n",
    "    sns.heatmap(data=ncm, fmt='.2f', annot=True)\n",
    "    \n",
    "plot_confusion_matrix(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ruh-roh...\n",
    "* That 40% accuracy rate is a little misleading\n",
    "* The classifier is performing this well by assigning the same classes everywhere\n",
    "* We need to fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.86      0.56     15571\n",
      "          1       0.00      0.00      0.00      3890\n",
      "          2       0.39      0.42      0.40      6803\n",
      "          3       0.00      0.00      0.00     13776\n",
      "\n",
      "avg / total       0.23      0.41      0.29     40040\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tal/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# The classification report shows us performance for\n",
    "# the most common metrics, by class\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iterating\n",
    "* Let's iteratively try to improve the model performance by:\n",
    "    * Adding features (we have plenty!)\n",
    "    * Making sure the estimator respects class weights\n",
    "    * Trying different estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84637862137862141"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For models with categoricals, we can call on\n",
    "# patsy to dummy-code our variables, like we did\n",
    "# when working with statsmodels\n",
    "from patsy import dmatrices\n",
    "\n",
    "# Some estimators we can try (there are many more!)\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# overfitting central!\n",
    "est = DecisionTreeClassifier()\n",
    "\n",
    "# Set up the features--let's go big this time\n",
    "X = data[['age']]\n",
    "\n",
    "# We already set up y, so we're just passing in 1 here\n",
    "# as a dummy. Note that we store it in a variable named\n",
    "# _ on the return, which is a convention to indicate\n",
    "# somethign we want to throw away. It's only the X's\n",
    "# we care about here.\n",
    "# _, X = dmatrices('1 ~ age', data=X)\n",
    "\n",
    "# Add in the features we extracted/selected\n",
    "# X = np.c_[X.data, selected_features]\n",
    "X = np.c_[X.values, lotsa_features]\n",
    "\n",
    "est.fit(X, y)\n",
    "y_pred = est.predict(X)\n",
    "\n",
    "# Be skeptical!\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "* Hopefully performance now looks reasonable\n",
    "* But there's still a potential problem: overfitting\n",
    "* We're training and evaluating on the same dataset--this is a big no-no!\n",
    "* scikit-learn provides easy ways to evaluate models out-of-sample\n",
    "    * This is known as cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.58702701  0.58969159  0.58941551  0.59317643  0.58348452]\n"
     ]
    }
   ],
   "source": [
    "# Import KFold cross-validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "est = DecisionTreeClassifier()\n",
    "\n",
    "# Placeholder for scores from each fold\n",
    "scores = []\n",
    "\n",
    "# Create k folds (in our case 5). Loop over the folds,\n",
    "# and for each one, split the dataset into training and test.\n",
    "# In each fold, we train the data on the training values,\n",
    "# and then evaluate its performance on the test. Finally,\n",
    "# we can take the average of the out-of-sample scores as\n",
    "# our estimate of model performance.\n",
    "folds = KFold(n=len(X), n_folds=5, shuffle=True)\n",
    "for train, test in folds:\n",
    "#     print(train, test)\n",
    "    est.fit(X[train], y[train])\n",
    "    pred_y = est.predict(X[test])\n",
    "    fold_score = f1_score(y[test], pred_y, average='weighted')\n",
    "    scores.append(fold_score)\n",
    "    \n",
    "# Overfitting be gone!\n",
    "print(np.array(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model selection\n",
    "* We've experimented a lot with estimators, decomposition, evaluation, etc.\n",
    "* This is only the tip of the iceberg...\n",
    "* scikit-learn has hundreds of estimators!\n",
    "* Two problems:\n",
    "    1. How are we supposed to choose?\n",
    "    2. How do we do this in a principled way?\n",
    "* Requirement for both is an automated, principled way to execute a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully automated pipelines\n",
    "* We've done feature extraction, reduction, and selection; estimation; evaluation...\n",
    "* But we need to automate this\n",
    "    * Both for efficiency, and to prevent overfitting\n",
    "* sklearn.pipeline provides functionality for creating [fully automated Pipelines](http://scikit-learn.org/stable/modules/pipeline.html)\n",
    "* We'll build a toy example with 2 steps, but we could chain our entire workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Initialize a list to store all the steps in our pipeline\n",
    "steps = []\n",
    "\n",
    "# Add feature selection\n",
    "selector = SelectKBest(k=100)\n",
    "steps.append(('select', selector))\n",
    "\n",
    "# Add estimation\n",
    "estimator = LogisticRegression()\n",
    "steps.append(('estimate', estimator))\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "_y_pred = pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The fit-predict interface\n",
    "* scikit-learn is built around the estimator interface\n",
    "* \"An estimator is an object that fits a model based on some training data and is capable of inferring some properties on new data\"\n",
    "* Every estimator must implement fit() and predict() methods\n",
    "* Makes it easy to extend scikit-learn with our own estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Building our own estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MercurialClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Picks a random class and assigns that label to all cases.\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Selects a random class from the available options '''\n",
    "        classes = np.unique(y)\n",
    "        self.selected_ = np.random.choice(classes)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Applies the selected class to everything '''\n",
    "        return np.repeat(self.selected_, len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End\n",
    "* I hope you enjoyed the course and/or learned something useful!\n",
    "* The links throughout the notebooks contain plenty of further resources\n",
    "* Consider registering for [SciPy 2016](scipy2016.scipy.org) (here in July)--they have great tutorials\n",
    "* Feedback/suggestions for improvement is welcome\n",
    "* Please fill out a course evaluation before you leave"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
